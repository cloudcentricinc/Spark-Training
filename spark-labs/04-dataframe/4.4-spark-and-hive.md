<link rel='stylesheet' href='../assets/css/main.css'/>

# 4.4: Spark And Hive


<img src="../assets/images/spark-and-hadoop-1.png" style="border: 5px solid grey ; max-width:100%;" />

<img src="../assets/images/spark-and-hadoop-2.png" style="border: 5px solid grey ; max-width:100%;" />


## Step 1 : Open Two Terminal Sessions to Hadoop Node
Instructor will demonstrate how.

## Step 2 : In Terminal 1 : Start Hive Shell
```bash
    $   hive
```

Inspect the tables and run a query on 'clickstream' table.

```sql
    hive>
        show tables;
          -- x_clickstream,  x_domains

        select * from x_clickstream limit 10;

        select action, count(*) as total from x_clickstream group by action;
```


## Step 3 : In Terminal 2 Start Spark shell

```bash
    $    spark-shell  --name "MYNAME spark shell"
```

### Controlling the UI options
Spark Shell by default publishes a UI on port number 4040.  
How ever when multiple apps are running, and port 4040 already taken, Spark Shell will try to find an open port (4041, 4042 ..etc)

Specifying a custom port
```bash
    $   spark-shell  --conf spark.ui.port=4060
```

Turn off UI altogether  
```bash
    $   spark-shell  --conf spark.ui.enabled=false
```

Type this in Spark Shell
```
    sc.setLogLevel("WARN")
```
Go to http://localhost:4040 in the browser.



## Step 4 : Inspect Hive Tables
Do this in Spark-Shell

```scala

scala>

    // hive top level tables
    sqlContext.tableNames
    val t = sqlContext.table("x_clickstream")
    t.printSchema
    t.show
    sqlContext.sql("select * from x_clickstream limit 10").show
    sqlContext.sql("select action, count(*) as total from x_clickstream group by action").show

    // accessing Hive databases (access using DBName.TabeName)
    // TODO : fix 'db_name' accordingly ('default')
    val t2 = sqlContext.table("db_name.x_domains")
    t2.printSchema
    t2.show
    sqlContext.sql("select * from db_name.x_domains limit 10").show

```

## Step 5 : Inspect YARN Resource Manager UI
Inspect Yarn RM UI.  Instructor will provide details.  
**=> Do you see the spark shell running as a YARN app?  why or why not?**

## Step 6:  Run Spark on the Hadoop cluster
When starting Spark shell, let's connect to the cluster as follows.

```bash
  $   spark-shell  --master yarn
```

**=> Inspect the YARN UI to see if Spark app is connected**
