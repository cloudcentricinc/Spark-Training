<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 3.1 : RDD Basics operations
================================

### Overview
* Learning basic operations like filter / map / count
* work with larger sized RDDs
* Load multiple files into a single RDD
* Save computed RDDs


### Depends On
None

### Run time
30-40 mins


## STEP 1: Basic RDD Operations

#### In Standalone env.
```bash
    $   ~/spark/bin/spark-shell
```

#### In Hadoop env
```bash
    $  spark-shell
```

### Controlling the UI options
Spark Shell by default publishes a UI on port number 4040.  
How ever when multiple apps are running, and port 4040 already taken, Spark Shell will try to find an open port (4041, 4042 ..etc)

Specifying a custom port
```bash
    $   spark-shell  --conf spark.ui.port=4060
```

Turn off UI altogether  
```bash
    $   spark-shell  --conf spark.ui.enabled=false
```

## Step 2: Load a small text file

```scala

scala>
    val f = sc.textFile("/data/text/twinkle/sample.txt")
```


**=> what is the 'type' of f ?**  
Hint : just type `f` in the shell
Here is a possible output

```scala
    scala> f
    res0: org.apache.spark.rdd.RDD[String] = /data/text/twinkle/sample.txt MappedRDD[3] at textFile at <console>:12
```


## STEP 3: Filter
Let's find how many lines contain the word 'twinkle'.  
We will use the 'filter' function

```scala
    val filtered = f.filter(line => line.contains("twinkle"))

    // short hand version
    val filtered = f.filter(_.contains("twinkle"))
```


After entering the above in Spark-shell...  
**=> Goto Spark shell UI (port number 4040)**  
**=> Inspect the 'Stages' section in the UI.**  
**=> How is the filter executed? Can you explain the behavior?**  

**=> Count how many lines contain the word 'twinkle'**  
hint : apply `count()` to `filtered` variable

Here is a sample output

```console
    15/03/31 23:19:30 INFO DAGScheduler: Stage 0 (count at <console>:17) finished in 0.074 s
    15/03/31 23:19:30 INFO DAGScheduler: Job 0 finished: count at <console>:17, took 0.141676 s
    res1: Long = 2  <--- this is the result of count()
```


**=> Check the Stages in UI,  what do you see?**  
**=> How long did the job take?**  
**=> Print out all the lines containing the word 'twinkle'**   
Hint : `collect()`

Here is a sample output
```console
res2: Array[String] = Array(twinkle twinkle little star, twinkle twinkle little star)
```

**=> Checkout 'DAG' visualization**

<img src="../assets/images/3.1c.png" style="border: 5px solid grey; max-width:100%;"/>


**=> Quit Spark-shell using 'exit'  or pressing  Control+D**


## STEP 4:  Large data sets
**==> Quit previous spark-shell session, if you haven't done so yet.. `Control + D`**  

We have some large data sets of 'twinkle' data generated in `/data/text/twinkle`  directory.

**=> Verify the data files and their sizes by doing the following**  
If you are on a standalone Spark environment:
```bash
    $   l /data/text/twinkle
```

If you are on Hadoop Spark environment:
```
  $ hdfs dfs -ls /data/text/twinkle/
```  
Your output might look like this

<img src="../assets/images/3.1a.png" style="border: 5px solid grey; max-width:100%;"/>

We are going to use these files in spark

**=> [Optional] You can use the following script to generate more data if you need to**  

```bash
    $   cd  ~/data/text/twinkle
    $   ./create-data-files.sh
```

This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)


## STEP 5:  Start shell With More Memory

```bash
    $   ~/spark/bin/spark-shell  --executor-memory 1G  --driver-memory 1G
```

## STEP 6: Process a large file
**=> In Spark Shell, load `/data/text/twinkle/100M.data`**  

```scala
scala>
    val f = sc.textFile("/data/text/twinkle/100M.data")
```

**=> Count number of lines that have the word "diamond"**  
hint : `filter`  and `count`
```scala
    val filtered = f.filter(_.contains("?? what is the word ??"))
```

**=> How many 'tasks' are used in the above calculation**  
hint : Check spark shell UI at port 4040
<img src="../assets/images/3.1b.png" style="border: 5px solid grey; max-width:100%;" />

**=> Can you explain the number of tasks?**  
Hint : check number of partitions in RDD using
```
    f.getNumPartitions
```



**=> Count number of lines that does NOT have the word 'diamond'**  
Hint : use negative operator  !  
`filter(line => ! line.contains("diamond")) `

**=> Verify both counts add up to the total line count**

**=> Notice the time taken for each operation**

**=> Try the above with larger data files : 500M.data  ... 1G.data**
  - note the times taken
  - how many tasks?


## STEP 7: Loading multiple files
**=> Load all *.data files under  /data/text/twinkle  directory**  
hint : `val files = sc.textFile("/data/text/twinkle/*.data")`

**=> Do a count() on RDD.**
Notice the partition count and time taken to execute
Verify partition count from Spark-Shell UI


## STEP 8:  Saving the RDD
Continuing with the big RDD created on step (7)....

**=> Create a new RDD by filtering first RDD for word 'diamond'**  
Hint : `filter`

**=> Save the new RDD into a directory**  
Hint :   `rdd.saveAsTextFile("MYNAME_out1")`  (Change MY_NAME accordingly)

**=> Inspect the console output during the run.**

**=> Inspect the output directory**  
In standalone Spark env,
```bash
    $   ls -lh   MY_NAME_out1  # change MY_NAME accordingly
    $   less  MYNAME_out1/part-00000
```

In Hadoop Spark Env:
```
  $  hdfs dfs -ls  MYNAME_out1
```
Or use HDFS File Browser


**=> What do you see as output?**


## Bonus Lab: Merging partitions into a single one
When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   
Hint : see the API for `coalesce or repartition`
