<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 3.1b : Dataset Basics operations
================================

- [Standalone version](3.1b-dataset-basics.md)
- [Hadoop version](3.1H-dataset-hadoop.md)

### Overview
* Working with Datasets
* Learning basic operations like filter / map / count
* work with larger sized data
* Save computed Dataset

### Depends On
None

### Run time
30-40 mins


## STEP 1:  Fire up Spark shell

```bash
    $   ~/spark/bin/spark-shell
```


## STEP 2: Load a simple text file

```scala
scala>
    val f = spark.read.textFile("/data/text/twinkle/sample.txt")

    // v1.6
    //val f = sc.textFile("/data/text/twinkle/sample.txt")
```


**=> what is the 'type' of f ?**  
Hint : just type `f` in the shell
Here is a possible output

```scala
    scala> f
    f: org.apache.spark.sql.Dataset[String] = [value: string]
```


## STEP 3: Filter
Let's find how many lines contain the word 'twinkle'
We will use the 'filter' function

```scala
    val filtered = f.filter(line => line.contains("twinkle"))

    // short hand version
    val filtered = f.filter(_.contains("twinkle"))
```

After entering the above in Spark-shell...  
**=> Goto Spark shell UI **  
**=> Inspect the 'Stages' section in the UI.**  
**=> How is the filter executed? Can you explain the behavior?**  

**=> Count how many lines contain the word 'twinkle'**  
hint : apply `count()` to `filtered` variable

Here is a sample output

```console
15/03/31 23:19:30 INFO DAGScheduler: Stage 0 (count at <console>:17) finished in 0.074 s
15/03/31 23:19:30 INFO DAGScheduler: Job 0 finished: count at <console>:17, took 0.141676 s
res1: Long = 2  <--- this is the result of count()
```


**=> Check the Stages in UI,  what do you see?**  
**=> How long did the job take?**  
**=> Print out all the lines containing the word 'twinkle'**   
Hint : `collect()`  
Here is a sample output
```console
res2: Array[String] = Array(twinkle twinkle little star, twinkle twinkle little star)
```

**=> Checkout 'DAG' visualization**  
Note this is different than what we saw with RDDs.  
<img src="../assets/images/3.1d.png" style="border: 5px solid grey; max-width:100%;"/>

**==> Try `explain` keyword**  

```scala
    f.explain
```

```console
*FileScan text [value#17] Batched: false, Format: Text,
Location: InMemoryFileIndex[file:/Users/sujee/data/text/twinkle/100M.data],
PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>
```



**=> Quit Spark-shell using 'exit'  or pressing  Control+D**


## STEP 4:  Large data sets
**==> Quit previous spark-shell session, if you haven't done so yet.. `Control + D`**  

We have some large data sets of 'twinkle' data generated in `/data/text/twinkle`  directory.

**=> Verify the data files and their sizes by doing a**
```
    $   l /data/text/twinkle
```
Your output might look like this

<img src="../assets/images/3.1a.png" style="border: 5px solid grey; max-width:100%;"/>

We are going to use these files in spark

**=> [Optional] You can use the following script to generate more data if you need to**  

    $   cd  /data/text/twinkle
    $   ./create-data-files.sh


This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)

## STEP 5:  Start shell With More Memory

```bash
    $   ~/spark/bin/spark-shell  --executor-memory 1G  --driver-memory 1G
```

## STEP 6: Process a large file
**=> In Spark Shell, load `/data/text/twinkle/100M.data`**  
```scala
scala>
    val f = spark.read.textFile("/data/text/twinkle/100M.data")
```

**=> Count number of lines that have the word "diamond"**  
hint : `filter`  and `count`

**=> How many 'tasks' are used in the above calculation**  
Hint : Check spark shell UI

<img src="../assets/images/3.1b.png" style="border: 5px solid grey; max-width:100%;" />

**=> Can you explain the number of tasks?**  
Hint : check number of partitions in Dataset using `getNumPartitions`  or `partitions.length`  
```scala

    # v2
    f.rdd.getNumPartitions

    # V1.6
    f.getNumPartitions
```


**=> Count number of lines that does NOT have the word 'diamond'**  
Hint : use negative operator  !  
`f.filter(line => ! line.contains("diamond")) `

**=> Verify both counts add up to the total line count**

**=> Notice the time taken for each operation**

**=> Try the above with larger data files : 500M.data  ... 1G.data**
  - note the times taken
  - how many tasks?

## STEP 7: Loading multiple files
**=> Load all *.data files under  /data/text/twinkle  directory**  
```scala
    val f = spark.read.textFile("/data/text/twinkle/*.data")
```

**=> Do a count() on **  
Notice the partition count and time taken to execute
Verify partition count from Spark-Shell UI

## STEP 8:  Saving the Dataset
Continuing with the big Dataset created on step (5)....

**=> Create a new Dataset by filtering first Dataset for word 'diamond'**  

```scala
    val filtered = f.filter(_.contains("diamond"))
```

**=> Save the new Dataset into a directory**  
```scala
    filtered.write.text("MYNAME_out1")  // adjust MYNAME accordingly
```

**=> Inspect the console output during the run.**

Inspect the output directory
In standalone Spark env, you can use
```
    $    ls -lh   MYNAME_out1
    $   less  out1/part-00000*
```

In Hadoop Spark env, you can use
```
    $    hdfs dfs -ls    MYNAME_out1
```

**=> What do you see as output?**


## Bonus Lab: Merging partitions into a single one
When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   
Hint : see the API for `coalesce or repartition`
