<link rel='stylesheet' href='../assets/css/main.css'/>

<< [back to main index](../README.md)

Lab 3.4: Map Reduce
===================
### Overview
Learn MapReduce in Spark step by step

### Depends On
None

### Run time
20 mins


--------------------------
STEP 1: Launch Spark Shell
--------------------------

    $   ~/spark/bin/spark-shell


**=> Make an RDD**

```scala
    // scala
    val input = Array ("hello world", "good bye world", "ok bye")
    val r = sc.makeRDD(input)    // this works in the same way as 'parallelize'
```
**=> Add up the words and counts**

------------
STEP 2 : Map
------------
**=> Apply `map()` function to rdd**  

    val r2 = r.map(line => line.split(" "))

**=> Print out the results by `collect()`**  
**=> Notice the resulting RDD type**

---------------
STEP 3: flatMap
---------------
**=> Use `flatMap` to create another RDD**

    val r3 = r.flatMap( line => line.split(" "))

**=> Print out the results, can you explain the data type**


----------------------
STEP 4: Create KV pair
----------------------
**=> Create a key-value pair by using `flatMap` and `map`**  

    val r4 = r.flatMap(line => line.split(" ")).map(word => (word, 1))

-------------------
STEP 5: reduceByKey
-------------------
**=> Add up the words and counts**

    val r5 = r4.reduceByKey((a,b) => a+b)


------------------------
STEP 6:  [Optional] Generating data
------------------------
If not there already, we can generate some data

    $   cd  /data/text/twinkle
    $   ./create-data-files.sh

This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)
Verify the data files and their sizes by doing a

    $   ls -lh

We are going to use these files as input


------------------------
STEP 7:  Do wordcount on larger file
------------------------
Load one of the larger files (100M + )

    val f = sc.textFile("file path")
    // do the wordcount
