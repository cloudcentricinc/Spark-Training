<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 5.1 First Spark Job Submission
==================================

### Overview
Submit a job for Spark

### Depends On
None

### Run time
20-30 mins


## Step 0 : Editing Files on VM
Please follow [these instructions](../edit-files.md)

## STEP 1: Edit source file

Go to the project root directory

```bash
    $    cd ~/spark-labs/05-api
```


**=> Edit file : `~/spark-labs/05-api/src/main/scala/x/ProcessFiles.scala`**  
**=> And fix the TODO items**


## STEP 2: Compile the project

We will use `sbt` to build the project.  

**=> Inspect the `build.sbt` file**

The file will look follows:

```scala
// blank lines are important!

name := "TestApp"

version := "1.0"

scalaVersion := "2.11.7"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "2.1.0" % "provided",
  "org.apache.spark" %% "spark-sql" % "2.1.0" % "provided"
)

// for accessing files from S3 or HDFS
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.0" exclude("com.google.guava", "guava")


```


**=> Kick off a build**  
(This will take a few minutes for the first time you run it)

```bash
    # be in the project root level directory
    $   cd   ~/spark-labs/05-api

    $   sbt package

    # to do a clean rebuild use
    $  sbt clean package
```

Make sure there are no errors and there is output in `target` dir.

```bash
    $  ls -l   target/scala-2.11
```

You should see output like follows

```console
    drwxr-xr-x  3 sujee  staff   102B Apr 16 09:59 classes/
    -rw-r--r--  1 sujee  staff    13K Apr 16 09:59 testapp_2.11-1.0.jar
```

`testapp_2.11-1.0.jar `  is our code compiled.


## STEP 3: Test Application in Local Master Mode

```bash
    $  cd  ~/spark-labs/05-api

    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    README.md
```

**==> Checkout the Shell UI (4040)**   

**==> Hit Enter key to terminate the program**

**==> Turn off the logs by sending logs by `2> logs` **   

```bash
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    README.md  2> logs
```

**==> Try a few input files **
```bash
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    /data/text/twinkle/*  2> logs
```


Now let's submit the application to Spark server

## STEP 4: Start Spark Server

```bash
    $  ~/spark/sbin/start-all.sh
```

**=> Check the Spark Server UI at port 8080.**  
**=> Note the Spark master URL.**  

<img src="../assets/images/4.1b.png" style="border: 5px solid grey; max-width:100%;"/>


## STEP 5: Submit the application

Use the following command to submit the job

```bash
    $  cd  ~/spark-labs/05-api

    # single README file
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    README.md   2> logs

    # multiple twinkle files
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    /data/text/twinkle/*  2> logs
```

* MASTER URL : substitute your spark master url
* for files you can try `README.md`

**=> Watch the console output**

It may look like this

```console
    15/04/15 15:41:51 INFO SparkUI: Started SparkUI at http://192.168.1.115:4040
    ...
    15/04/15 15:41:54 INFO DAGScheduler: Job 0 finished: count at ProcessFiles.scala:42, took 2.156893 s

    ### README.md : count (7) took 2,251.007000 ms
```

The lines starting with `###` are output from our program


## STEP 6:  Configuring logging

#### To quickly turn off the logging:
Redirect the logs as follows `  2> logs`.   
All logs will be sent to `logs` file.  
```bash
    $  ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    <files to process>    2>  logs
```

#### Using log4j config
Spark by default logs at INFO level.  While there is a lot of useful information there, it can be quite verbose.

We have a `logging/log4j.properties` file.  Inspect this file

```bash
    $    cat   logging/log4j.properties
```


The file has following contents

```
# Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
```



We provide `--driver-class-path logging/`  to spark-submit to turn off logging

Here is an example

```bash
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master local[*]  --driver-class-path logging/  target/scala-2.11/testapp_2.11-1.0.jar    README.md
```


## STEP 7:  Bonus Lab : Read a file from Amazon S3

For the file argument, try the following `s3n://elephantscale-public/data/text/twinkle/100M.data`

For example:

```bash
    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles'   --driver-class-path logging/  target/scala-2.11/testapp_2.11-1.0.jar    's3n://elephantscale-public/data/text/twinkle/100M.data'
```


**=> Notice the time it too to access the file**

**=> To find out more data files on S3 use the following command**

```bash
    $   s3cmd ls s3://elephantscale-public/data/text/twinkle/
```
